"""
ä»…æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½
"""


import sys
import os
# å°† backend ç›®å½•æ”¾åˆ° sys.path é¦–ä½ï¼Œç¡®ä¿ä¼˜å…ˆä½¿ç”¨æœ¬åœ° services åŒ…
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.text_chunker import RecursiveTextChunker


def test_text_chunking():
    """æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½"""
    
    print("ğŸ”„ å¼€å§‹æµ‹è¯•æ–‡æœ¬åˆ†å—...")
    
    # åˆ›å»ºåˆ†å—å™¨
    chunker = RecursiveTextChunker(
        chunk_size=512,
        chunk_overlap=100
    )
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
    å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
    è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
    
    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒæ˜¯ä¸€ç§è®©è®¡ç®—æœºç³»ç»Ÿè‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›çš„æ–¹æ³•ï¼Œ
    æ— éœ€è¢«æ˜ç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚
    
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
    æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚
    
    è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸï¼Œ
    å®ƒè‡´åŠ›äºè®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPæŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“ã€
    æœºå™¨ç¿»è¯‘ã€è¯­éŸ³åŠ©æ‰‹ã€æ–‡æœ¬åˆ†æç­‰é¢†åŸŸã€‚
    
    è®¡ç®—æœºè§†è§‰æ˜¯å¦ä¸€ä¸ªé‡è¦çš„AIåº”ç”¨é¢†åŸŸï¼Œå®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­æå–ã€
    åˆ†æå’Œç†è§£æœ‰ç”¨ä¿¡æ¯ã€‚è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦è¯Šæ–­ã€å®‰é˜²ç›‘æ§ã€
    å·¥ä¸šè´¨æ£€ç­‰é¢†åŸŸå‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚
    """
    
    metadata = {
        "title": "äººå·¥æ™ºèƒ½ä»‹ç»",
        "category": "technology",
        "author": "æµ‹è¯•ä½œè€…"
    }
    
    try:
        # 1. æµ‹è¯• token è®¡ç®—
        total_tokens = chunker.count_tokens(test_text)
        print(f"ğŸ“Š åŸæ–‡æ€» token æ•°: {total_tokens}")
        print(f"ğŸ“Š åŸæ–‡å­—ç¬¦æ•°: {len(test_text)}")
        
        # 2. æµ‹è¯•æ–‡æœ¬åˆ†å—
        chunks = chunker.chunk_text(test_text, metadata)
        
        print(f"\nğŸ“ åˆ†å—ç»“æœ:")
        print(f"  åˆ†å—æ•°é‡: {len(chunks)}")
        
        total_chunk_tokens = 0
        for i, chunk in enumerate(chunks):
            print(f"\n  åˆ†å— {i}:")
            print(f"    Token æ•°: {chunk.token_count}")
            print(f"    å­—ç¬¦æ•°: {len(chunk.content)}")
            print(f"    ä½ç½®: {chunk.start_pos}-{chunk.end_pos}")
            print(f"    å†…å®¹é¢„è§ˆ: {chunk.content[:100]}...")
            
            total_chunk_tokens += chunk.token_count
        
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»åˆ†å— tokens: {total_chunk_tokens}")
        print(f"  å¹³å‡æ¯å— tokens: {total_chunk_tokens / len(chunks):.1f}")
        
        # 3. æ£€æŸ¥é‡å æƒ…å†µ
        if len(chunks) > 1:
            print(f"\nğŸ”— é‡å æ£€æŸ¥:")
            for i in range(len(chunks) - 1):
                current_end = chunks[i].content[-50:]  # å½“å‰å—ç»“å°¾
                next_start = chunks[i + 1].content[:50]  # ä¸‹ä¸€å—å¼€å¤´
                print(f"  å— {i}-{i+1} è¾¹ç•Œ:")
                print(f"    å½“å‰å—ç»“å°¾: ...{current_end}")
                print(f"    ä¸‹ä¸€å—å¼€å¤´: {next_start}...")
        
        print("\nâœ… æ–‡æœ¬åˆ†å—æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_text_chunking()
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)