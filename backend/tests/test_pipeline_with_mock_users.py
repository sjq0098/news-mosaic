#!/usr/bin/env python3
"""
Pipelineå®Œæ•´åŠŸèƒ½æµ‹è¯• - ä½¿ç”¨æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®

æ­¤æµ‹è¯•å±•ç¤ºå¦‚ä½•ä¸ºä¸åŒç±»å‹çš„ç”¨æˆ·åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼Œå¹¶æµ‹è¯•pipelineçš„å„ç§åŠŸèƒ½
"""

import asyncio
import time
import uuid
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from loguru import logger

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.complete_pipeline_service import (
    CompletePipelineService, 
    PipelineRequest, 
    PipelineMode
)
from services.conversation_context_manager import ConversationContextManager
from models.user_memory import (
    UserMemoryProfile, UserPreference, MemoryItem, MemoryType,
    InterestCategory, ConversationContext
)
from models.chat import ChatMessage, MessageRole


class MockUserDataGenerator:
    """æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®ç”Ÿæˆå™¨"""
    
    @staticmethod
    def create_tech_enthusiast_user(user_id: str = None) -> UserMemoryProfile:
        """åˆ›å»ºç§‘æŠ€çˆ±å¥½è€…ç”¨æˆ·"""
        user_id = user_id or f"tech_user_{uuid.uuid4().hex[:8]}"
        
        preferences = UserPreference(
            preferred_categories=[
                InterestCategory.TECHNOLOGY,
                InterestCategory.SCIENCE,
                InterestCategory.BUSINESS
            ],
            disliked_categories=[InterestCategory.ENTERTAINMENT],
            preferred_news_length="long",
            preferred_analysis_depth="comprehensive",
            communication_style="professional",
            response_format="detailed"
        )
        
        # æ·»åŠ ä¸€äº›æŠ€æœ¯ç›¸å…³çš„è®°å¿†
        memories = [
            MemoryItem(
                id=str(uuid.uuid4()),
                user_id=user_id,
                memory_type=MemoryType.PREFERENCE,
                content="ç”¨æˆ·å¯¹AIå’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ç‰¹åˆ«æ„Ÿå…´è¶£",
                importance_score=0.9,
                related_topics=["AI", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
            ),
            MemoryItem(
                id=str(uuid.uuid4()),
                user_id=user_id,
                memory_type=MemoryType.INTERACTION,
                content="ç»å¸¸è¯¢é—®æŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œåº”ç”¨å‰æ™¯",
                importance_score=0.8,
                related_topics=["æŠ€æœ¯è¶‹åŠ¿", "åº”ç”¨åœºæ™¯"]
            ),
            MemoryItem(
                id=str(uuid.uuid4()),
                user_id=user_id,
                memory_type=MemoryType.KNOWLEDGE,
                content="å…·å¤‡ä¸€å®šçš„ç¼–ç¨‹èƒŒæ™¯ï¼Œç†è§£æŠ€æœ¯æ¦‚å¿µ",
                importance_score=0.7,
                related_topics=["ç¼–ç¨‹", "æŠ€æœ¯ç†è§£"]
            )
        ]
        
        return UserMemoryProfile(
            user_id=user_id,
            preferences=preferences,
            memories=memories,
            total_conversations=15,
            total_memories=len(memories),
            personalization_score=0.8,
            interaction_frequency=0.7
        )
    
    @staticmethod
    def create_business_analyst_user(user_id: str = None) -> UserMemoryProfile:
        """åˆ›å»ºå•†ä¸šåˆ†æå¸ˆç”¨æˆ·"""
        user_id = user_id or f"business_user_{uuid.uuid4().hex[:8]}"
        
        preferences = UserPreference(
            preferred_categories=[
                InterestCategory.FINANCE,
                InterestCategory.BUSINESS,
                InterestCategory.INTERNATIONAL
            ],
            disliked_categories=[InterestCategory.SPORTS],
            preferred_news_length="medium",
            preferred_analysis_depth="detailed",
            communication_style="professional",
            response_format="structured"
        )
        
        memories = [
            MemoryItem(
                id=str(uuid.uuid4()),
                user_id=user_id,
                memory_type=MemoryType.PREFERENCE,
                content="å…³æ³¨å¸‚åœºåŠ¨å‘å’Œä¼ä¸šè´¢åŠ¡è¡¨ç°",
                importance_score=0.9,
                related_topics=["å¸‚åœºåˆ†æ", "è´¢åŠ¡", "ä¼ä¸šå‘å±•"]
            ),
            MemoryItem(
                id=str(uuid.uuid4()),
                user_id=user_id,
                memory_type=MemoryType.CONTEXT,
                content="ç»å¸¸éœ€è¦æ•°æ®æ”¯æ’‘çš„åˆ†ææŠ¥å‘Š",
                importance_score=0.8,
                related_topics=["æ•°æ®åˆ†æ", "æŠ¥å‘Š", "è¶‹åŠ¿é¢„æµ‹"]
            )
        ]
        
        return UserMemoryProfile(
            user_id=user_id,
            preferences=preferences,
            memories=memories,
            total_conversations=25,
            total_memories=len(memories),
            personalization_score=0.9,
            interaction_frequency=0.8
        )
    
    @staticmethod
    def create_general_user(user_id: str = None) -> UserMemoryProfile:
        """åˆ›å»ºæ™®é€šç”¨æˆ·"""
        user_id = user_id or f"general_user_{uuid.uuid4().hex[:8]}"
        
        preferences = UserPreference(
            preferred_categories=[
                InterestCategory.LIFESTYLE,
                InterestCategory.HEALTH,
                InterestCategory.LOCAL
            ],
            preferred_news_length="short",
            preferred_analysis_depth="brief",
            communication_style="casual",
            response_format="simple"
        )
        
        memories = [
            MemoryItem(
                id=str(uuid.uuid4()),
                user_id=user_id,
                memory_type=MemoryType.PREFERENCE,
                content="åå¥½ç®€æ´æ˜äº†çš„ä¿¡æ¯",
                importance_score=0.6,
                related_topics=["ç®€æ´", "æ˜“æ‡‚"]
            )
        ]
        
        return UserMemoryProfile(
            user_id=user_id,
            preferences=preferences,
            memories=memories,
            total_conversations=5,
            total_memories=len(memories),
            personalization_score=0.4,
            interaction_frequency=0.3
        )
    
    @staticmethod
    def create_conversation_context(session_id: str, user_id: str, conversation_type: str = "general") -> ConversationContext:
        """åˆ›å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if conversation_type == "tech":
            return ConversationContext(
                session_id=session_id,
                user_id=user_id,
                current_topic="äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•",
                discussed_topics=["AI", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "],
                mentioned_entities=["GPT", "Transformer", "ç¥ç»ç½‘ç»œ"],
                user_questions=["AIæŠ€æœ¯çš„å‘å±•å‰æ™¯å¦‚ä½•ï¼Ÿ", "æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ"],
                conversation_sentiment="positive",
                complexity_level="high",
                message_count=8
            )
        elif conversation_type == "business":
            return ConversationContext(
                session_id=session_id,
                user_id=user_id,
                current_topic="å¸‚åœºåˆ†æ",
                discussed_topics=["è‚¡å¸‚", "ç»æµè¶‹åŠ¿", "è¡Œä¸šåˆ†æ"],
                mentioned_entities=["GDP", "CPI", "è‚¡ç¥¨æŒ‡æ•°"],
                user_questions=["å½“å‰å¸‚åœºè¶‹åŠ¿å¦‚ä½•ï¼Ÿ", "å“ªäº›è¡Œä¸šå€¼å¾—å…³æ³¨ï¼Ÿ"],
                conversation_sentiment="neutral",
                complexity_level="medium",
                message_count=5
            )
        else:
            return ConversationContext(
                session_id=session_id,
                user_id=user_id,
                current_topic="æ—¥å¸¸æ–°é—»",
                discussed_topics=["ç”Ÿæ´»", "å¥åº·"],
                mentioned_entities=[],
                user_questions=["æœ€è¿‘æœ‰ä»€ä¹ˆæœ‰è¶£çš„æ–°é—»å—ï¼Ÿ"],
                conversation_sentiment="neutral",
                complexity_level="low",
                message_count=2
            )


class PipelineTestSuite:
    """Pipelineæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.pipeline_service = CompletePipelineService()
        self.context_manager = ConversationContextManager()
        self.test_results = []
    
    async def setup_mock_users(self):
        """è®¾ç½®æ¨¡æ‹Ÿç”¨æˆ·"""
        logger.info("ğŸ”§ è®¾ç½®æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®...")
        
        # åˆ›å»ºä¸åŒç±»å‹çš„ç”¨æˆ·
        self.tech_user = MockUserDataGenerator.create_tech_enthusiast_user()
        self.business_user = MockUserDataGenerator.create_business_analyst_user()
        self.general_user = MockUserDataGenerator.create_general_user()
        
        # æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_manager.user_profiles[self.tech_user.user_id] = self.tech_user
        self.context_manager.user_profiles[self.business_user.user_id] = self.business_user
        self.context_manager.user_profiles[self.general_user.user_id] = self.general_user
        
        logger.info(f"âœ… å·²åˆ›å»º 3 ä¸ªæ¨¡æ‹Ÿç”¨æˆ·:")
        logger.info(f"   â€¢ ç§‘æŠ€ç”¨æˆ·: {self.tech_user.user_id}")
        logger.info(f"   â€¢ å•†ä¸šç”¨æˆ·: {self.business_user.user_id}")
        logger.info(f"   â€¢ æ™®é€šç”¨æˆ·: {self.general_user.user_id}")
    
    async def test_personalized_responses(self):
        """æµ‹è¯•ä¸ªæ€§åŒ–å›å¤"""
        logger.info("\nğŸ“Š æµ‹è¯•1: ä¸ªæ€§åŒ–å›å¤åŠŸèƒ½")
        print("-" * 60)
        
        # åŒä¸€ä¸ªé—®é¢˜ï¼Œä¸åŒç”¨æˆ·åº”è¯¥å¾—åˆ°ä¸åŒé£æ ¼çš„å›å¤
        query = "æœ€è¿‘AIæŠ€æœ¯æœ‰ä»€ä¹ˆæ–°çªç ´ï¼Ÿ"
        
        test_users = [
            ("ç§‘æŠ€ç”¨æˆ·", self.tech_user),
            ("å•†ä¸šç”¨æˆ·", self.business_user),
            ("æ™®é€šç”¨æˆ·", self.general_user)
        ]
        
        for user_type, user_profile in test_users:
            print(f"\nğŸ”¹ {user_type} ({user_profile.user_id}):")
            
            request = PipelineRequest(
                user_id=user_profile.user_id,
                message=query,
                mode=PipelineMode.ENHANCED_CHAT,
                enable_memory=True,
                enable_rag=True,
                enable_cards=False
            )
            
            start_time = time.time()
            response = await self.pipeline_service.process_pipeline(request)
            execution_time = time.time() - start_time
            
            print(f"   ğŸ’¬ AIå›å¤: {response.ai_response[:200]}...")
            print(f"   â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
            print(f"   ğŸ¯ ä¸ªæ€§åŒ–å¾—åˆ†: {response.confidence_score:.2f}")
            print(f"   âœ… æˆåŠŸ: {response.success}")
            
            self.test_results.append({
                "test": "ä¸ªæ€§åŒ–å›å¤",
                "user_type": user_type,
                "success": response.success,
                "execution_time": execution_time,
                "confidence": response.confidence_score
            })
    
    async def test_different_pipeline_modes(self):
        """æµ‹è¯•ä¸åŒçš„Pipelineæ¨¡å¼"""
        logger.info("\nğŸ”§ æµ‹è¯•2: ä¸åŒPipelineæ¨¡å¼")
        print("-" * 60)
        
        user_id = self.tech_user.user_id
        query = "æ–°èƒ½æºæ±½è½¦å¸‚åœºå‘å±•å¦‚ä½•ï¼Ÿ"
        
        # æµ‹è¯•ä¸åŒæ¨¡å¼
        modes = [
            (PipelineMode.ENHANCED_CHAT, "ä»…å¢å¼ºå¯¹è¯"),
            (PipelineMode.RAG_ANALYSIS, "RAGåˆ†æ"),
            (PipelineMode.CARD_GENERATION, "å¡ç‰‡ç”Ÿæˆ"),
            (PipelineMode.UNIFIED_COMPLETE, "å®Œæ•´ç»Ÿä¸€"),
            (PipelineMode.CUSTOM, "è‡ªå®šä¹‰æ¨¡å¼")
        ]
        
        for mode, description in modes:
            print(f"\nğŸ”¹ æµ‹è¯•æ¨¡å¼: {description}")
            
            request = PipelineRequest(
                user_id=user_id,
                message=query,
                mode=mode,
                enable_memory=True,
                enable_rag=True,
                enable_cards=(mode == PipelineMode.CARD_GENERATION or mode == PipelineMode.UNIFIED_COMPLETE)
            )
            
            start_time = time.time()
            try:
                response = await self.pipeline_service.process_pipeline(request)
                execution_time = time.time() - start_time
                
                print(f"   âœ… æˆåŠŸ: {response.success}")
                print(f"   â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
                print(f"   ğŸ“° æ£€ç´¢æ–°é—»æ•°: {len(response.retrieved_news)}")
                print(f"   ğŸ´ ç”Ÿæˆå¡ç‰‡æ•°: {len(response.generated_cards)}")
                
                self.test_results.append({
                    "test": "Pipelineæ¨¡å¼",
                    "mode": mode,
                    "success": response.success,
                    "execution_time": execution_time,
                    "news_count": len(response.retrieved_news),
                    "cards_count": len(response.generated_cards)
                })
                
            except Exception as e:
                print(f"   âŒ å¤±è´¥: {str(e)}")
                self.test_results.append({
                    "test": "Pipelineæ¨¡å¼",
                    "mode": mode,
                    "success": False,
                    "error": str(e)
                })
    
    async def test_conversation_context(self):
        """æµ‹è¯•å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†"""
        logger.info("\nğŸ§  æµ‹è¯•3: å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†")
        print("-" * 60)
        
        user_id = self.tech_user.user_id
        session_id = f"test_session_{uuid.uuid4().hex[:8]}"
        
        # åˆ›å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = MockUserDataGenerator.create_conversation_context(
            session_id, user_id, "tech"
        )
        self.context_manager.active_contexts[session_id] = conversation_context
        
        # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
        conversation_turns = [
            "åˆšæ‰æˆ‘ä»¬èŠåˆ°äº†AIæŠ€æœ¯ï¼Œèƒ½è¯¦ç»†è¯´è¯´GPTçš„å‘å±•å†ç¨‹å—ï¼Ÿ",
            "é‚£Transformeræ¶æ„æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Ÿ",
            "è¿™äº›æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å¦‚ä½•ï¼Ÿ"
        ]
        
        for i, message in enumerate(conversation_turns, 1):
            print(f"\nğŸ”„ ç¬¬{i}è½®å¯¹è¯:")
            print(f"   ğŸ‘¤ ç”¨æˆ·: {message}")
            
            request = PipelineRequest(
                user_id=user_id,
                session_id=session_id,
                message=message,
                mode=PipelineMode.ENHANCED_CHAT,
                enable_memory=True,
                enable_rag=True
            )
            
            start_time = time.time()
            response = await self.pipeline_service.process_pipeline(request)
            execution_time = time.time() - start_time
            
            print(f"   ğŸ¤– AI: {response.ai_response[:150]}...")
            print(f"   â±ï¸ æ—¶é—´: {execution_time:.2f}ç§’")
            
            # æ¨¡æ‹Ÿæ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
            conversation_context.message_count += 2  # ç”¨æˆ·+AI
            conversation_context.last_updated_at = datetime.utcnow()
            
            self.test_results.append({
                "test": "å¯¹è¯ä¸Šä¸‹æ–‡",
                "turn": i,
                "success": response.success,
                "execution_time": execution_time
            })
    
    async def test_batch_processing(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†"""
        logger.info("\nâš¡ æµ‹è¯•4: æ‰¹é‡å¤„ç†åŠŸèƒ½")
        print("-" * 60)
        
        # åˆ›å»ºå¤šä¸ªç”¨æˆ·çš„å¹¶å‘è¯·æ±‚
        batch_requests = [
            (self.tech_user.user_id, "AIæŠ€æœ¯çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ"),
            (self.business_user.user_id, "å½“å‰è‚¡å¸‚è¡¨ç°å¦‚ä½•ï¼Ÿ"),
            (self.general_user.user_id, "æœ€è¿‘æœ‰ä»€ä¹ˆæœ‰è¶£çš„æ–°é—»ï¼Ÿ"),
            (self.tech_user.user_id, "é‡å­è®¡ç®—çš„å®ç”¨åŒ–è¿›å±•æ€æ ·ï¼Ÿ"),
            (self.business_user.user_id, "æ–°èƒ½æºæ±½è½¦è¡Œä¸šçš„æŠ•èµ„æœºä¼šåœ¨å“ªé‡Œï¼Ÿ")
        ]
        
        # åˆ›å»ºè¯·æ±‚å¯¹è±¡
        pipeline_requests = []
        for user_id, message in batch_requests:
            request = PipelineRequest(
                user_id=user_id,
                message=message,
                mode=PipelineMode.UNIFIED_COMPLETE,
                enable_memory=True,
                enable_rag=True,
                enable_cards=True
            )
            pipeline_requests.append(request)
        
        # å¹¶å‘æ‰§è¡Œ
        print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(pipeline_requests)} ä¸ªè¯·æ±‚...")
        
        start_time = time.time()
        results = await asyncio.gather(*[
            self.pipeline_service.process_pipeline(req) 
            for req in pipeline_requests
        ], return_exceptions=True)
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        success_count = sum(1 for r in results if isinstance(r, object) and hasattr(r, 'success') and r.success)
        error_count = len(results) - success_count
        
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"   â€¢ æ€»è¯·æ±‚æ•°: {len(pipeline_requests)}")
        print(f"   â€¢ æˆåŠŸæ•°: {success_count}")
        print(f"   â€¢ å¤±è´¥æ•°: {error_count}")
        print(f"   â€¢ æ€»æ—¶é—´: {total_time:.2f}ç§’")
        print(f"   â€¢ å¹³å‡æ—¶é—´: {total_time/len(pipeline_requests):.2f}ç§’")
        
        self.test_results.append({
            "test": "æ‰¹é‡å¤„ç†",
            "total_requests": len(pipeline_requests),
            "success_count": success_count,
            "error_count": error_count,
            "total_time": total_time,
            "average_time": total_time/len(pipeline_requests)
        })
    
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        logger.info("\nğŸ”¥ æµ‹è¯•5: é”™è¯¯å¤„ç†æœºåˆ¶")
        print("-" * 60)
        
        # æµ‹è¯•å„ç§é”™è¯¯æƒ…å†µ
        error_cases = [
            ("ç©ºæ¶ˆæ¯", ""),
            ("è¶…é•¿æ¶ˆæ¯", "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æ¶ˆæ¯" * 100),
            ("ä¸å­˜åœ¨çš„ç”¨æˆ·", "æµ‹è¯•æ¶ˆæ¯"),
            ("æ— æ•ˆæ¨¡å¼", "æµ‹è¯•æ¶ˆæ¯")
        ]
        
        for case_name, message in error_cases:
            print(f"\nğŸ”¹ é”™è¯¯æµ‹è¯•: {case_name}")
            
            try:
                user_id = "invalid_user" if case_name == "ä¸å­˜åœ¨çš„ç”¨æˆ·" else self.tech_user.user_id
                
                request = PipelineRequest(
                    user_id=user_id,
                    message=message,
                    mode=PipelineMode.UNIFIED_COMPLETE if case_name != "æ— æ•ˆæ¨¡å¼" else "invalid_mode"
                )
                
                start_time = time.time()
                response = await self.pipeline_service.process_pipeline(request)
                execution_time = time.time() - start_time
                
                print(f"   âœ… å¤„ç†å®Œæˆ: {response.success}")
                print(f"   â±ï¸ æ—¶é—´: {execution_time:.2f}ç§’")
                
            except Exception as e:
                print(f"   âŒ å¼‚å¸¸æ•è·: {str(e)[:100]}...")
                
            self.test_results.append({
                "test": "é”™è¯¯å¤„ç†",
                "case": case_name,
                "handled": True
            })
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("\nğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        print("ğŸ¯ PipelineåŠŸèƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„
        test_groups = {}
        for result in self.test_results:
            test_type = result["test"]
            if test_type not in test_groups:
                test_groups[test_type] = []
            test_groups[test_type].append(result)
        
        # è¾“å‡ºå„ç»„ç»“æœ
        for test_type, results in test_groups.items():
            print(f"\nğŸ“Š {test_type}:")
            print("-" * 40)
            
            if test_type == "ä¸ªæ€§åŒ–å›å¤":
                for r in results:
                    status = "âœ…" if r["success"] else "âŒ"
                    print(f"   {status} {r['user_type']}: {r['execution_time']:.2f}s, ç½®ä¿¡åº¦: {r['confidence']:.2f}")
            
            elif test_type == "Pipelineæ¨¡å¼":
                for r in results:
                    if r["success"]:
                        print(f"   âœ… {r['mode']}: {r['execution_time']:.2f}s, æ–°é—»:{r['news_count']}, å¡ç‰‡:{r['cards_count']}")
                    else:
                        print(f"   âŒ {r['mode']}: {r.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            elif test_type == "å¯¹è¯ä¸Šä¸‹æ–‡":
                success_rate = sum(1 for r in results if r["success"]) / len(results) * 100
                avg_time = sum(r["execution_time"] for r in results) / len(results)
                print(f"   ğŸ¯ æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"   â±ï¸ å¹³å‡æ—¶é—´: {avg_time:.2f}ç§’")
            
            elif test_type == "æ‰¹é‡å¤„ç†":
                r = results[0]  # åªæœ‰ä¸€ä¸ªæ‰¹é‡æµ‹è¯•ç»“æœ
                print(f"   ğŸ“¦ è¯·æ±‚æ•°: {r['total_requests']}")
                print(f"   âœ… æˆåŠŸç‡: {r['success_count']}/{r['total_requests']} ({r['success_count']/r['total_requests']*100:.1f}%)")
                print(f"   âš¡ ååé‡: {r['total_requests']/r['total_time']:.2f} è¯·æ±‚/ç§’")
            
            elif test_type == "é”™è¯¯å¤„ç†":
                handled_count = sum(1 for r in results if r["handled"])
                print(f"   ğŸ›¡ï¸ é”™è¯¯å¤„ç†è¦†ç›–: {handled_count}/{len(results)} ({handled_count/len(results)*100:.1f}%)")
        
        # æ€»ç»“
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r.get("success", True))
        
        print(f"\nğŸ† æµ‹è¯•æ€»ç»“:")
        print(f"   â€¢ æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   â€¢ æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"   â€¢ æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        print(f"   â€¢ åŠŸèƒ½è¦†ç›–: ä¸ªæ€§åŒ–ã€å¤šæ¨¡å¼ã€ä¸Šä¸‹æ–‡ã€æ‰¹é‡å¤„ç†ã€é”™è¯¯å¤„ç†")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Pipelineå®Œæ•´åŠŸèƒ½æµ‹è¯•")
    print(f"â° æµ‹è¯•å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    test_suite = PipelineTestSuite()
    
    try:
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        await test_suite.setup_mock_users()
        
        # è¿è¡Œæµ‹è¯•
        await test_suite.test_personalized_responses()
        await test_suite.test_different_pipeline_modes()
        await test_suite.test_conversation_context()
        await test_suite.test_batch_processing()
        await test_suite.test_error_handling()
        
        # ç”ŸæˆæŠ¥å‘Š
        test_suite.generate_test_report()
        
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ! ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 