"""
QWen Embedding ç”ŸæˆæœåŠ¡
"""

import asyncio
import time
import uuid
from typing import List, Dict, Any, Optional
import numpy as np
from loguru import logger

# LangChain ç»„ä»¶
from langchain.text_splitter import RecursiveCharacterTextSplitter
# ä½¿ç”¨é€šä¹‰ DashScope Embedding
from langchain_community.embeddings.dashscope import DashScopeEmbeddings

from core.config import settings
from models.embedding import TextChunk, EmbeddingResult, EmbeddingResultModel


class QWenEmbeddingService:
    """QWen Embedding æœåŠ¡"""

    def __init__(self):
        self.api_key = settings.QWEN_API_KEY
        self.base_url = settings.QWEN_BASE_URL
        self.model_name = "text-embedding-v3"

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¼”ç¤ºæ¨¡å¼
        self.demo_mode = not settings.is_api_configured("qwen")

        if self.demo_mode:
            logger.warning("âš ï¸ EmbeddingæœåŠ¡è¿è¡Œåœ¨æ¼”ç¤ºæ¨¡å¼ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡")

        # ä½¿ç”¨ LangChain çš„é€’å½’å¼åˆ†å—å™¨ï¼ˆä¸åŸé€»è¾‘ä¿æŒ chunk_size/overlap ä¸€è‡´ï¼‰
        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=512,
            chunk_overlap=100,
            encoding_name="cl100k_base"
        )

        # åªåœ¨éæ¼”ç¤ºæ¨¡å¼ä¸‹åˆå§‹åŒ–çœŸå®çš„embeddingæ¨¡å‹
        if not self.demo_mode:
            self.embedding_model = DashScopeEmbeddings(
                model=self.model_name,
                dashscope_api_key=self.api_key,
            )
        else:
            self.embedding_model = None

        self.batch_size = 10  # æ‰¹é‡å¤„ç†å¤§å°
    
    async def chunk_text(
        self, 
        text: str, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[TextChunk]:
        """
        æ–‡æœ¬åˆ†å—
        
        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            metadata: å…ƒæ•°æ®
            
        Returns:
            æ–‡æœ¬åˆ†å—åˆ—è¡¨
        """
        try:
            start_time = time.time()
            # ä½¿ç”¨ LangChain splitter
            docs = self.text_splitter.create_documents([text], metadatas=[metadata or {}])

            # è½¬æ¢ä¸ºå†…éƒ¨ TextChunk ç»“æ„
            chunks = []
            for i, doc in enumerate(docs):
                # è¿™é‡Œæ— æ³•ç›´æ¥è·å–å­—ç¬¦ä½ç½®ï¼Œç®€å•ä½¿ç”¨é€’å¢ç´¢å¼•
                chunks.append(TextChunk(
                    content=doc.page_content,
                    chunk_index=i,
                    token_count=self.text_splitter._length_function(doc.page_content),
                    start_pos=-1,
                    end_pos=-1,
                    metadata=doc.metadata
                ))
            
            processing_time = time.time() - start_time
            
            logger.info(
                f"æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªåˆ†å—, "
                f"è€—æ—¶: {processing_time:.2f}s"
            )
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†å—å¤±è´¥: {e}")
            raise
    
    async def generate_embedding(self, text: str) -> np.ndarray:
        """
        ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„ embedding

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            embedding å‘é‡
        """
        if self.demo_mode:
            # æ¼”ç¤ºæ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿå‘é‡
            logger.debug(f"ğŸ­ æ¼”ç¤ºæ¨¡å¼ï¼šä¸ºæ–‡æœ¬ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡")
            return np.array(self._generate_mock_embedding(text), dtype=np.float32)

        # ä½¿ç”¨ LangChain åŒæ­¥æ¥å£ï¼Œåœ¨å¼‚æ­¥ç¯å¢ƒä¸‹å€ŸåŠ© asyncio.to_thread
        vector = await asyncio.to_thread(self.embedding_model.embed_query, text)
        return np.array(vector, dtype=np.float32)
    
    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        è·å–æ–‡æœ¬çš„embeddingå‘é‡ - å…¼å®¹æ€§æ–¹æ³•
        """
        try:
            if not texts:
                return []

            if self.demo_mode:
                # æ¼”ç¤ºæ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿå‘é‡
                logger.info(f"ğŸ­ æ¼”ç¤ºæ¨¡å¼ï¼šä¸º {len(texts)} ä¸ªæ–‡æœ¬ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡")
                return [self._generate_mock_embedding(text) for text in texts]

            embeddings = await self.generate_embeddings_batch(texts)
            # è½¬æ¢ä¸ºList[List[float]]æ ¼å¼
            return [embedding.tolist() if hasattr(embedding, 'tolist') else list(embedding) for embedding in embeddings]
        except Exception as e:
            logger.error(f"è·å–embeddingså¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä¹Ÿè¿”å›æ¨¡æ‹Ÿå‘é‡
            return [self._generate_mock_embedding(text) for text in texts]
    
    async def generate_embeddings_batch(self, texts: List[str]) -> List[np.ndarray]:
        """
        æ‰¹é‡ç”Ÿæˆ embeddings

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            embedding å‘é‡åˆ—è¡¨
        """
        if self.demo_mode:
            # æ¼”ç¤ºæ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿå‘é‡
            logger.info(f"ğŸ­ æ¼”ç¤ºæ¨¡å¼ï¼šæ‰¹é‡ç”Ÿæˆ {len(texts)} ä¸ªæ¨¡æ‹Ÿå‘é‡")
            return [np.array(self._generate_mock_embedding(text), dtype=np.float32) for text in texts]

        # LangChain embed_documents æ˜¯åŒæ­¥å‡½æ•°ï¼ŒåŒæ ·ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥åŒ–
        vectors = await asyncio.to_thread(self.embedding_model.embed_documents, texts)
        return [np.array(v, dtype=np.float32) for v in vectors]

    def _generate_mock_embedding(self, text: str) -> List[float]:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰
        åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆç¡®å®šæ€§çš„å‘é‡ï¼Œç¡®ä¿ç›¸ä¼¼æ–‡æœ¬æœ‰ç›¸ä¼¼å‘é‡
        """
        # ä½¿ç”¨æ–‡æœ¬çš„hashå€¼ä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬ç”Ÿæˆç›¸åŒå‘é‡
        import hashlib
        text_hash = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        np.random.seed(text_hash % (2**32))

        # ç”Ÿæˆ1536ç»´å‘é‡ï¼ˆä¸é€šä¹‰åƒé—®embeddingç»´åº¦ä¸€è‡´ï¼‰
        vector = np.random.normal(0, 1, 1536).astype(np.float32)

        # å½’ä¸€åŒ–å‘é‡
        vector = vector / np.linalg.norm(vector)

        return vector.tolist()
    
    async def process_text(
        self,
        text: str,
        source_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[EmbeddingResult]:
        """
        å®Œæ•´å¤„ç†æ–‡æœ¬ï¼šåˆ†å— + ç”Ÿæˆ embeddings
        
        Args:
            text: å¾…å¤„ç†çš„æ–‡æœ¬
            source_id: æ¥æº ID
            metadata: å…ƒæ•°æ®
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        try:
            start_time = time.time()
            
            # 1. æ–‡æœ¬åˆ†å—
            chunks = await self.chunk_text(text, metadata)
            
            if not chunks:
                logger.warning("æ–‡æœ¬åˆ†å—ç»“æœä¸ºç©º")
                return []
            
            # 2. æ‰¹é‡ç”Ÿæˆ embeddings
            chunk_texts = [chunk.content for chunk in chunks]
            results = []
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(chunk_texts), self.batch_size):
                batch_texts = chunk_texts[i:i + self.batch_size]
                batch_chunks = chunks[i:i + self.batch_size]
                
                batch_start_time = time.time()
                embeddings = await self.generate_embeddings_batch(batch_texts)
                batch_processing_time = time.time() - batch_start_time
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                for chunk, embedding in zip(batch_chunks, embeddings):
                    result = EmbeddingResult(
                        chunk=chunk,
                        embedding=embedding,
                        model_info={
                            "model": self.model_name,
                            "version": "v3",
                            "dimension": len(embedding),
                            "source_id": source_id
                        },
                        processing_time=batch_processing_time / len(batch_texts)
                    )
                    results.append(result)
                
                logger.info(
                    f"æ‰¹æ¬¡ {i//self.batch_size + 1} å¤„ç†å®Œæˆ: "
                    f"{len(batch_texts)} ä¸ªåˆ†å—, è€—æ—¶: {batch_processing_time:.2f}s"
                )
            
            total_processing_time = time.time() - start_time
            logger.info(
                f"æ–‡æœ¬å¤„ç†å®Œæˆ: {len(results)} ä¸ªåˆ†å—, "
                f"æ€»è€—æ—¶: {total_processing_time:.2f}s"
            )
            
            return results
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
            raise
    
    async def process_texts_batch(
        self,
        texts: List[str],
        source_ids: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None
    ) -> List[List[EmbeddingResult]]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            source_ids: æ¥æº ID åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æ¯ä¸ªæ–‡æœ¬çš„å¤„ç†ç»“æœåˆ—è¡¨
        """
        if len(texts) != len(source_ids):
            raise ValueError("æ–‡æœ¬æ•°é‡å’Œæ¥æº ID æ•°é‡ä¸åŒ¹é…")
        
        if metadatas and len(metadatas) != len(texts):
            raise ValueError("å…ƒæ•°æ®æ•°é‡å’Œæ–‡æœ¬æ•°é‡ä¸åŒ¹é…")
        
        if metadatas is None:
            metadatas = [{}] * len(texts)
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡æœ¬
        tasks = [
            self.process_text(text, source_id, metadata)
            for text, source_id, metadata in zip(texts, source_ids, metadatas)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ–‡æœ¬ {source_ids[i]} å¤„ç†å¤±è´¥: {result}")
                final_results.append([])
            else:
                final_results.append(result)
        
        return final_results
    
    async def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        # å…¼å®¹æ–°ç‰ˆ LangChainï¼Œå±æ€§å¯èƒ½æ”¹ä¸º _chunk_size/_chunk_overlap
        chunk_size = getattr(self.text_splitter, "chunk_size", getattr(self.text_splitter, "_chunk_size", None))
        chunk_overlap = getattr(self.text_splitter, "chunk_overlap", getattr(self.text_splitter, "_chunk_overlap", None))

        return {
            "model_name": self.model_name,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "batch_size": self.batch_size,
            "api_endpoint": self.base_url
        }
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        pass


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
embedding_service = QWenEmbeddingService()