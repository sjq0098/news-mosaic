"""
é›†æˆRAGæœåŠ¡ - æ—¢ç”Ÿæˆæ–°é—»å¡ç‰‡åˆè¿›è¡Œæ™ºèƒ½å¯¹è¯
"""

import asyncio
import time
from typing import List, Dict, Any, Optional
from loguru import logger

from services.qwen_service import QWENService
from services.embedding_service import QWenEmbeddingService
from services.rag_enhanced_card_service import RAGEnhancedCardService
from models.news import NewsModel
from models.news_card import NewsCardRequest, NewsCardResponse
from mock_data.news_samples import mock_news_db


class IntegratedRAGService:
    """é›†æˆRAGæœåŠ¡ - æ–°é—»å¡ç‰‡ç”Ÿæˆ + æ™ºèƒ½å¯¹è¯"""
    
    def __init__(self):
        self.qwen_service = QWENService()
        self.embedding_service = QWenEmbeddingService()
        self.card_service = RAGEnhancedCardService()
        
        # æ¨¡æ‹Ÿå‘é‡å­˜å‚¨
        self.vector_store = {}
        self.news_embeddings = {}
        
        logger.info("é›†æˆRAGæœåŠ¡å·²åˆå§‹åŒ–")
    
    async def initialize_pipeline(self):
        """åˆå§‹åŒ–æµæ°´çº¿ - ä¸ºæ‰€æœ‰æ–°é—»ç”Ÿæˆembedding"""
        logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–é›†æˆRAGæµæ°´çº¿...")
        
        all_news = mock_news_db.get_all_news()
        logger.info(f"ğŸ“° åŠ è½½äº† {len(all_news)} æ¡æ¨¡æ‹Ÿæ–°é—»")
        
        for i, news_data in enumerate(all_news):
            await self._process_news_embedding(news_data)
            logger.info(f"âœ… å¤„ç†æ–°é—» {i+1}/{len(all_news)}: {news_data['title'][:50]}...")
        
        logger.info(f"ğŸ¯ é›†æˆRAGæµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼å…±å¤„ç† {len(all_news)} æ¡æ–°é—»")
    
    async def _process_news_embedding(self, news_data: Dict[str, Any]):
        """ä¸ºå•æ¡æ–°é—»ç”Ÿæˆembedding"""
        news_id = news_data["id"]
        
        # ç»„åˆæ–‡æœ¬ï¼šæ ‡é¢˜ + å†…å®¹æ‘˜è¦
        combined_text = f"{news_data['title']}\n\n{news_data['content'][:500]}..."
        
        try:
            # ç”Ÿæˆembedding
            embedding = await self.embedding_service.generate_embedding(combined_text)
            
            # å­˜å‚¨embeddingå’Œå…ƒæ•°æ®
            self.news_embeddings[news_id] = {
                "embedding": embedding,
                "text": combined_text,
                "news_data": news_data
            }
            
        except Exception as e:
            logger.error(f"ä¸ºæ–°é—» {news_id} ç”Ÿæˆembeddingå¤±è´¥: {e}")
    
    async def search_relevant_news(
        self, 
        query: str, 
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """åŸºäºæŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–°é—»"""
        try:
            # ä¸ºæŸ¥è¯¢ç”Ÿæˆembedding
            query_embedding = await self.embedding_service.generate_embedding(query)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            
            for news_id, stored_data in self.news_embeddings.items():
                similarity = self._cosine_similarity(
                    query_embedding, 
                    stored_data["embedding"]
                )
                
                similarities.append({
                    "news_id": news_id,
                    "similarity": similarity,
                    "news_data": stored_data["news_data"]
                })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_k
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            
            relevant_news = []
            for item in similarities[:top_k]:
                relevant_news.append({
                    "news_data": item["news_data"],
                    "similarity_score": item["similarity"]
                })
            
            logger.info(f"ğŸ” æ£€ç´¢åˆ° {len(relevant_news)} æ¡ç›¸å…³æ–°é—»")
            return relevant_news
            
        except Exception as e:
            logger.error(f"æ£€ç´¢ç›¸å…³æ–°é—»å¤±è´¥: {e}")
            return []
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            import numpy as np
            
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm_vec1 = np.linalg.norm(vec1)
            norm_vec2 = np.linalg.norm(vec2)
            
            if norm_vec1 == 0 or norm_vec2 == 0:
                return 0.0
            
            return dot_product / (norm_vec1 * norm_vec2)
            
        except Exception as e:
            logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    async def generate_news_card(self, news_data: Dict[str, Any]) -> Optional[NewsCardResponse]:
        """ç”Ÿæˆæ–°é—»å¡ç‰‡ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        try:
            # è½¬æ¢ä¸ºNewsModelæ ¼å¼
            from models.news import NewsSource
            news_model = NewsModel(
                id=news_data["id"],
                title=news_data["title"],
                content=news_data["content"],
                source=NewsSource.MANUAL,  # ä½¿ç”¨MANUALä½œä¸ºé»˜è®¤æ¥æº
                url=news_data["url"],
                published_at=news_data["published_at"]
            )
            
            # åˆ›å»ºè¯·æ±‚
            request = NewsCardRequest(
                news_id=news_data["id"],
                include_sentiment=True,
                include_entities=True,
                include_summary=True
            )
            
            # ä½¿ç”¨ä¿®å¤åçš„å¡ç‰‡æœåŠ¡ç”Ÿæˆå¡ç‰‡
            card_response = await self.card_service.generate_card_with_rag(news_model, request)
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ–°é—»å¡ç‰‡: {news_data['id']}")
            return card_response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–°é—»å¡ç‰‡å¤±è´¥: {e}")
            return None
    
    async def chat_with_news_context(
        self, 
        user_query: str,
        max_context_news: int = 3
    ) -> Dict[str, Any]:
        """åŸºäºæ–°é—»ä¸Šä¸‹æ–‡è¿›è¡Œå¯¹è¯"""
        try:
            start_time = time.time()
            
            # 1. æ£€ç´¢ç›¸å…³æ–°é—»
            relevant_news = await self.search_relevant_news(user_query, top_k=max_context_news)
            
            if not relevant_news:
                logger.warning("æœªæ‰¾åˆ°ç›¸å…³æ–°é—»ï¼Œä½¿ç”¨é€šç”¨å›å¤")
                return {
                    "user_query": user_query,
                    "ai_response": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„æ–°é—»ä¿¡æ¯ã€‚",
                    "relevant_news": [],
                    "news_card": None,
                    "processing_time": time.time() - start_time
                }
            
            # 2. ä¸ºæœ€ç›¸å…³çš„æ–°é—»ç”Ÿæˆå¡ç‰‡
            top_news = relevant_news[0]["news_data"]
            news_card = await self.generate_news_card(top_news)
            
            # 3. æ„å»ºæ–°é—»ä¸Šä¸‹æ–‡
            news_context = self._build_news_context(relevant_news)
            
            # 4. æ„å»ºå®Œæ•´æç¤º
            full_prompt = self._build_chat_prompt(user_query, news_context)
            
            # 5. è°ƒç”¨QWENç”Ÿæˆå›å¤
            response = await self.qwen_service.generate_response(
                user_message=full_prompt,
                chat_history=[],
                include_news=False,
                temperature=0.7,
                max_tokens=800
            )
            
            # 6. ç»„ç»‡è¿”å›ç»“æœ
            result = {
                "user_query": user_query,
                "ai_response": response.content,
                "relevant_news": [
                    {
                        "title": news["news_data"]["title"],
                        "source": news["news_data"]["source"],
                        "similarity": news["similarity_score"],
                        "category": news["news_data"]["category"],
                        "url": news["news_data"]["url"]
                    }
                    for news in relevant_news
                ],
                "news_card": {
                    "card_id": news_card.card.metadata.card_id if news_card else None,
                    "summary": news_card.card.metadata.summary if news_card else None,
                    "importance_level": news_card.card.metadata.importance_level.value if news_card else None,
                    "sentiment_label": news_card.card.metadata.sentiment_label.value if news_card else None,
                    "generation_time": news_card.processing_time if news_card else None
                } if news_card else None,
                "tokens_used": response.tokens_used,
                "generation_time": response.generation_time,
                "processing_time": time.time() - start_time
            }
            
            logger.info(f"ğŸ’¬ å®Œæ•´å¯¹è¯æµç¨‹å®Œæˆï¼Œè€—æ—¶: {result['processing_time']:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"åŸºäºæ–°é—»ä¸Šä¸‹æ–‡å¯¹è¯å¤±è´¥: {e}")
            return {
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def _build_news_context(self, relevant_news: List[Dict[str, Any]]) -> str:
        """æ„å»ºæ–°é—»ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, news_item in enumerate(relevant_news, 1):
            news = news_item["news_data"]
            similarity = news_item["similarity_score"]
            
            context_part = f"""
æ–°é—» {i} (ç›¸ä¼¼åº¦: {similarity:.3f}):
æ ‡é¢˜: {news['title']}
æ¥æº: {news['source']}
åˆ†ç±»: {news['category']}
å†…å®¹æ‘˜è¦: {news['content'][:400]}...
å‘å¸ƒæ—¶é—´: {news['published_at'].strftime('%Y-%m-%d %H:%M')}
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def _build_chat_prompt(self, user_query: str, news_context: str) -> str:
        """æ„å»ºå¯¹è¯æç¤º"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ç›¸å…³æ–°é—»ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç›¸å…³æ–°é—»ä¿¡æ¯:
{news_context}

ç”¨æˆ·é—®é¢˜: {user_query}

è¯·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„åˆ†æå›å¤ã€‚å›å¤åº”è¯¥:
1. åŸºäºæä¾›çš„æ–°é—»ä¿¡æ¯
2. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥è°¨
3. å®¢è§‚ä¸­æ€§ï¼Œé¿å…ä¸»è§‚æ¨æµ‹
4. å¦‚æœæ¶‰åŠå¤šæ¡æ–°é—»ï¼Œå¯ä»¥è¿›è¡Œå¯¹æ¯”åˆ†æ

å›å¤:
"""

    async def get_pipeline_status(self) -> Dict[str, Any]:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        return {
            "total_news": len(self.news_embeddings),
            "service_status": "ready",
            "categories": list(set([
                news_data["news_data"]["category"] 
                for news_data in self.news_embeddings.values()
            ])),
            "features": [
                "æ–°é—»å‘é‡æ£€ç´¢",
                "ç»“æ„åŒ–å¡ç‰‡ç”Ÿæˆ", 
                "æ™ºèƒ½å¯¹è¯å›å¤",
                "æ•°æ®æ ¼å¼æ˜ å°„",
                "å¹¶å‘å¤„ç†æ”¯æŒ"
            ]
        }


# åˆ›å»ºå…¨å±€å®ä¾‹
integrated_rag = IntegratedRAGService() 